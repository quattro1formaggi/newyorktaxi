{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa6f1b6d-0dc7-429b-99b1-115b5d0c2c44",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26ae6de1-126c-4aaf-aef2-6074e1254cc7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "        .appName('mllib') \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5aa41757-960f-4968-9b53-48d3261a07fe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "storage_account_name = \"chengyangbde\"\n",
    "storage_account_access_key = \"89Y9ZLqYL8RN3MNqcTnvMGNnPOuArORjnNDbWLhkXzkRTYaD/GfvSmaY8sdFz+wonSC4llRg5osD+AStJmIHug==\"\n",
    "blob_container_name = \"bde-2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd537e41-ae25-43d0-b803-c3c48c24b23a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mount_points = [mount.mountPoint for mount in dbutils.fs.mounts()]\n",
    "\n",
    "if f\"/mnt/{blob_container_name}/\" not in mount_points:\n",
    "    dbutils.fs.mount(\n",
    "      source = f'wasbs://{blob_container_name}@{storage_account_name}.blob.core.windows.net',\n",
    "      mount_point = f'/mnt/{blob_container_name}/',\n",
    "      extra_configs = {'fs.azure.account.key.' + storage_account_name + '.blob.core.windows.net': storage_account_access_key}\n",
    "    )\n",
    "else:\n",
    "    print(f\"/mnt/{blob_container_name}/ is already mounted. Skipping.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c44d856b-909e-476d-808b-da5de23c84f2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2749248832212531>, line 49\u001B[0m\n",
       "\u001B[1;32m     47\u001B[0m dt_model \u001B[38;5;241m=\u001B[39m pipeline_dt\u001B[38;5;241m.\u001B[39mfit(train_df)\n",
       "\u001B[1;32m     48\u001B[0m dt_predictions \u001B[38;5;241m=\u001B[39m dt_model\u001B[38;5;241m.\u001B[39mtransform(validation_df)\n",
       "\u001B[0;32m---> 49\u001B[0m dt_rmse \u001B[38;5;241m=\u001B[39m evaluator\u001B[38;5;241m.\u001B[39mevaluate(dt_predictions)\n",
       "\u001B[1;32m     50\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDecision Tree Regression Model RMSE on Validation Data: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdt_rmse\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.2f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m     52\u001B[0m dt_predictions_test \u001B[38;5;241m=\u001B[39m dt_model\u001B[38;5;241m.\u001B[39mtransform(test_df)\n",
       "\n",
       "\u001B[0;31mNameError\u001B[0m: name 'evaluator' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-2749248832212531>, line 49\u001B[0m\n\u001B[1;32m     47\u001B[0m dt_model \u001B[38;5;241m=\u001B[39m pipeline_dt\u001B[38;5;241m.\u001B[39mfit(train_df)\n\u001B[1;32m     48\u001B[0m dt_predictions \u001B[38;5;241m=\u001B[39m dt_model\u001B[38;5;241m.\u001B[39mtransform(validation_df)\n\u001B[0;32m---> 49\u001B[0m dt_rmse \u001B[38;5;241m=\u001B[39m evaluator\u001B[38;5;241m.\u001B[39mevaluate(dt_predictions)\n\u001B[1;32m     50\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDecision Tree Regression Model RMSE on Validation Data: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdt_rmse\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.2f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     52\u001B[0m dt_predictions_test \u001B[38;5;241m=\u001B[39m dt_model\u001B[38;5;241m.\u001B[39mtransform(test_df)\n\n\u001B[0;31mNameError\u001B[0m: name 'evaluator' is not defined",
       "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name 'evaluator' is not defined",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression, DecisionTreeRegressor\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.functions import col, lit, month, year\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"Taxi Fare Prediction\").getOrCreate()\n",
    "\n",
    "# Read the dataset from the Parquet file\n",
    "parquet_path = \"/mnt/bde-2/combined_data_cleaned3.parquet\"\n",
    "df = spark.read.parquet(parquet_path)\n",
    "\n",
    "# Compute trip duration in seconds\n",
    "df = df.withColumn(\"trip_duration\", (col(\"tpep_dropoff_datetime\").cast(\"long\") - col(\"tpep_pickup_datetime\").cast(\"long\")))\n",
    "\n",
    "# Fill null values with defaults\n",
    "default_values = {\n",
    "    \"trip_distance\": 0.0,\n",
    "    \"passenger_count\": 1,\n",
    "    \"RatecodeID\": 1,\n",
    "    \"trip_duration\": 0\n",
    "}\n",
    "df = df.fillna(default_values)\n",
    "\n",
    "# Separate data for test set before sampling\n",
    "test_df = df.filter((year(\"tpep_pickup_datetime\") == 2022) & (month(\"tpep_pickup_datetime\") >= 10))\n",
    "\n",
    "# Sample data for training and validation\n",
    "df_sampled = df.sample(False, 0.6, seed=42)\n",
    "\n",
    "# Split the sampled data into training and validation\n",
    "train_df, validation_df = df_sampled.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Load the Q3C answer from the CSV file\n",
    "q3c_df = spark.read.csv(\"/mnt/bde-2/Q3Canswer.csv\", header=True, inferSchema=True)\n",
    "average_fare_q3c = q3c_df.collect()[0][0]\n",
    "\n",
    "# Vectorize the features without the location IDs\n",
    "features = [\"trip_distance\", \"passenger_count\", \"RatecodeID\", \"trip_duration\"]\n",
    "vector_assembler = VectorAssembler(inputCols=features, outputCol=\"features\", handleInvalid=\"keep\")\n",
    "\n",
    "# Decision Tree Regression Model\n",
    "dt = DecisionTreeRegressor(featuresCol=\"features\", labelCol=\"total_amount\")\n",
    "pipeline_dt = Pipeline(stages=[vector_assembler, dt])\n",
    "dt_model = pipeline_dt.fit(train_df)\n",
    "dt_predictions = dt_model.transform(validation_df)\n",
    "dt_rmse = evaluator.evaluate(dt_predictions)\n",
    "print(f\"Decision Tree Regression Model RMSE on Validation Data: {dt_rmse:.2f}\")\n",
    "\n",
    "dt_predictions_test = dt_model.transform(test_df)\n",
    "dt_rmse_test = evaluator.evaluate(dt_predictions_test)\n",
    "print(f\"Decision Tree Regression Model RMSE on Test Data: {dt_rmse_test:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44a5958f-2c87-439e-b120-cb010c22360f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Regression Model RMSE on Validation Data: 127.22\nDecision Tree Regression Model RMSE on Test Data: 7.22\n"
     ]
    }
   ],
   "source": [
    "evaluator = RegressionEvaluator(labelCol=\"total_amount\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "dt_rmse = evaluator.evaluate(dt_predictions)\n",
    "print(f\"Decision Tree Regression Model RMSE on Validation Data: {dt_rmse:.2f}\")\n",
    "\n",
    "dt_predictions_test = dt_model.transform(test_df)\n",
    "dt_rmse_test = evaluator.evaluate(dt_predictions_test)\n",
    "print(f\"Decision Tree Regression Model RMSE on Test Data: {dt_rmse_test:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ASS2 Decision Tree",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
